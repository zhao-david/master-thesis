{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pylab\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import time\n",
    "import sklearn.decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.lda import LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "\n",
    "def load_mnist(dataset=\"training\", digits=np.arange(10),\n",
    "               path=r'C:\\Users\\David\\Documents\\ETHZ 2015-2017\\'16 HERBST\\THESIS\\MNIST'):\n",
    "    \"\"\"\n",
    "    Loads MNIST files into 3D numpy arrays\n",
    "\n",
    "    Adapted from: http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_train, labels_train = load_mnist(dataset=\"training\")\n",
    "images_test, labels_test = load_mnist(dataset=\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCANet transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eigendecomp(cov_matrix, L1):\n",
    "    eigenvals, eigenvecs = np.linalg.eig(cov_matrix)\n",
    "    idx = eigenvals.argsort()[::-1]   \n",
    "    eigenval = eigenvals[idx]\n",
    "    eigenvecs = eigenvecs[:,idx]\n",
    "    return eigenvecs[:,0:L1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: different from what is used in SCNet\n",
    "def convolution2D(in1, in2, subsample=1):\n",
    "    raw_out = scipy.signal.convolve2d(in1, in2, mode='full', boundary='fill', fillvalue=0)\n",
    "    \n",
    "    # trim so that output has desired dimensions (assume in1 is image, in2 is filter)\n",
    "    shape = np.shape(in2)\n",
    "    trim_size_x = np.floor(shape[0] / 2)\n",
    "    trim_size_y = np.floor(shape[1] / 2)\n",
    "    trimmed_out = raw_out[trim_size_x:-trim_size_x, trim_size_y:-trim_size_y]\n",
    "    \n",
    "    # subsample the trimmed output\n",
    "    out = trimmed_out[::subsample, ::subsample].copy()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(image):\n",
    "    return ((image > 0) / 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram(image, L2):\n",
    "    return np.bincount(image.flatten(), minlength=2**L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_6000_subset = pd.Series.from_csv('random_6000_subset.csv').values\n",
    "validation = random_6000_subset\n",
    "training = np.setdiff1d(np.arange(6000), validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0: 500 images up to index 500 took: 19.613677978515625 secs!\n",
      "LAYER 0: 500 images up to index 1000 took: 16.142269134521484 secs!\n",
      "LAYER 0: 500 images up to index 1500 took: 23.47346305847168 secs!\n",
      "LAYER 0: 500 images up to index 2000 took: 20.85823678970337 secs!\n",
      "LAYER 0: 500 images up to index 2500 took: 17.967917919158936 secs!\n",
      "LAYER 0: 500 images up to index 3000 took: 6.65828800201416 secs!\n",
      "LAYER 0: 500 images up to index 3500 took: 7.594223976135254 secs!\n",
      "LAYER 0: 500 images up to index 4000 took: 11.354557991027832 secs!\n",
      "LAYER 0: 500 images up to index 4500 took: 16.802999019622803 secs!\n",
      "LAYER 0: 500 images up to index 5000 took: 16.636905908584595 secs!\n",
      "Cov and eigendecomp 1 took 5.734333038330078 secs!\n",
      "LAYER 1: 500 images up to index 500 took: 54.58187198638916 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 72.70730900764465 secs!\n",
      "LAYER 1: 500 images up to index 1500 took: 76.78368210792542 secs!\n",
      "LAYER 1: 500 images up to index 2000 took: 71.16183185577393 secs!\n",
      "LAYER 1: 500 images up to index 2500 took: 75.76171803474426 secs!\n",
      "LAYER 1: 500 images up to index 3000 took: 91.3417809009552 secs!\n",
      "LAYER 1: 500 images up to index 3500 took: 155.2506399154663 secs!\n",
      "LAYER 1: 500 images up to index 4000 took: 186.70918703079224 secs!\n",
      "LAYER 1: 500 images up to index 4500 took: 167.14814805984497 secs!\n",
      "LAYER 1: 500 images up to index 5000 took: 155.03345489501953 secs!\n",
      "Cov and eigendecomp 2 took 8415.486681938171 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 115.6966700553894 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 289.74574398994446 secs!\n",
      "LAYER 2: 500 images up to index 1500 took: 62.47843599319458 secs!\n",
      "LAYER 2: 500 images up to index 2000 took: 48.919798135757446 secs!\n",
      "LAYER 2: 500 images up to index 2500 took: 53.138283014297485 secs!\n",
      "LAYER 2: 500 images up to index 3000 took: 47.112486124038696 secs!\n",
      "LAYER 2: 500 images up to index 3500 took: 47.87383413314819 secs!\n",
      "LAYER 2: 500 images up to index 4000 took: 44.21310496330261 secs!\n",
      "LAYER 2: 500 images up to index 4500 took: 36.28260111808777 secs!\n",
      "LAYER 2: 500 images up to index 5000 took: 27.80360698699951 secs!\n",
      "(5000, 18432)\n",
      "LAYER 1: 500 images up to index 500 took: 119.12278008460999 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 127.90145206451416 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 29.440436124801636 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 14.095057010650635 secs!\n",
      "(1000, 18432)\n"
     ]
    }
   ],
   "source": [
    "PCA_coeffs_train, PCA_filters_1, PCA_filters_2 = PCANet_train(images_train[training], B=9)\n",
    "np.savetxt('PCANet_train_new.txt', PCA_coeffs_train, delimiter=\",\")\n",
    "print(np.shape(PCA_coeffs_train))\n",
    "\n",
    "PCA_coeffs_test = PCANet_test(images_train[validation], PCA_filters_1, PCA_filters_2, B=9)\n",
    "np.savetxt('PCANet_test_new.txt', PCA_coeffs_test, delimiter=\",\")\n",
    "print(np.shape(PCA_coeffs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 1: 500 images up to index 500 took: 112.55853509902954 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 96.78524589538574 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 33.23877215385437 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 28.30772304534912 secs!\n",
      "(1000, 18432)\n"
     ]
    }
   ],
   "source": [
    "PCA_coeffs_test = PCANet_test(images_train[validation], PCA_filters_1, PCA_filters_2, B=9)\n",
    "np.savetxt('PCANet_test_new.txt', PCA_coeffs_test, delimiter=\",\")\n",
    "print(np.shape(PCA_coeffs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PCANet_train(images, k1=7, k2=7, L1=8, L2=8, B=1):\n",
    "    \n",
    "    out = []\n",
    "    X = []\n",
    "    X_L1 = []\n",
    "    images_L1 = []\n",
    "    pad_1 = np.floor(k1 / 2)\n",
    "    pad_2 = np.floor(k2 / 2)\n",
    "    N = np.shape(images)[0]\n",
    "    shape_1 = np.shape(images)[1]\n",
    "    shape_2 = np.shape(images)[2]\n",
    "    \n",
    "    # LAYER 0\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        padded_image = np.lib.pad(image, ((pad_1,pad_1),(pad_2,pad_2)), 'constant', constant_values=0)\n",
    "        for i in range(shape_1):\n",
    "            for j in range(shape_2):\n",
    "                patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                patch_flat = patch_i_j.flatten()\n",
    "                patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                X.append(patch_mean_rm)\n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 0: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X = np.reshape(X, (N*(shape_1)*(shape_2), k1*k2))\n",
    "    \n",
    "    cov_X = X.T.dot(X)\n",
    "    \n",
    "    filters_X_L1 = eigendecomp(cov_X, L1)\n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 1 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 1\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        for k in range(L1):\n",
    "            this_filter = np.reshape(filters_X_L1[:,k], (k1,k2))\n",
    "            image_L1 = convolution2D(image, this_filter)\n",
    "            images_L1.append(image_L1)\n",
    "            padded_image = np.lib.pad(image_L1, ((pad_1,pad_1),(pad_2,pad_2)), 'constant',\n",
    "                                      constant_values=0)\n",
    "            for i in range(shape_1-1):\n",
    "                for j in range(shape_2-1):\n",
    "                    patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                    patch_flat = patch_i_j.flatten()\n",
    "                    patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                    X_L1.append(patch_mean_rm)\n",
    "                    \n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 1: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X_L1 = np.reshape(X_L1, (N*L1*(shape_1-1)*(shape_2-1), k1*k2))\n",
    "    \n",
    "    cov_X_L1 = X_L1.T.dot(X_L1)\n",
    "    \n",
    "    filters_X_L2 = eigendecomp(cov_X_L1, L2)\n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 2 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 2\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    counter_L1 = 0\n",
    "    image_coeffs = []\n",
    "    for image_L1 in images_L1:\n",
    "        \n",
    "        counter_L1 += 1\n",
    "        \n",
    "        bin_image_L1 = np.zeros((shape_1, shape_2))\n",
    "        \n",
    "        for k in range(L2):\n",
    "            this_filter = np.reshape(filters_X_L2[:,k], (k1,k2))\n",
    "            \n",
    "            # output of the convolutional step: N*L1*L2 images\n",
    "            image_L2 = convolution2D(image_L1, this_filter)\n",
    "            \n",
    "            # binarization\n",
    "            bin_image_L2 = binarize(image_L2)\n",
    "            \n",
    "            # concatenation of binaries\n",
    "            bin_image_L1 += bin_image_L2 * 2**k\n",
    "        \n",
    "        # divide each of N*L1 images into B regions\n",
    "        # take histogram\n",
    "        bin_image_L1 = bin_image_L1.astype(int)\n",
    "        \n",
    "        if B == 1:\n",
    "            hist = histogram(bin_image_L1, L2)\n",
    "            image_coeffs.append(hist)\n",
    "        elif B == 9:\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    hist = histogram(bin_image_L1[9*a:9*a+9, 9*b:9*b+9], L2)\n",
    "                    image_coeffs.append(hist)\n",
    "        \n",
    "        # group each set of B*L1*2^L2 coefficients per image\n",
    "        if counter_L1 % L1 == 0:\n",
    "            \n",
    "            # final set of PCANet coeffs\n",
    "            out.append(image_coeffs)\n",
    "            image_coeffs = []\n",
    "            \n",
    "            t1 = time.time()\n",
    "            t += 1\n",
    "            if t % 500 == 0:\n",
    "                print('LAYER 2: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "                t0 = time.time()\n",
    "    \n",
    "    return np.array(out).reshape(N, B*L1*2**L2), filters_X_L1, filters_X_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PCANet_test(images, filters_X_L1, filters_X_L2, k1=7, k2=7, L1=8, L2=8, B=9):\n",
    "    \n",
    "    out = []\n",
    "    images_L1 = []\n",
    "    pad_1 = np.floor(k1 / 2)\n",
    "    pad_2 = np.floor(k2 / 2)\n",
    "    N = np.shape(images)[0]\n",
    "    shape_1 = np.shape(images)[1]\n",
    "    shape_2 = np.shape(images)[2]\n",
    "    \n",
    "    # LAYER 1\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        for k in range(L1):\n",
    "            this_filter = np.reshape(filters_X_L1[:,k], (k1,k2))\n",
    "            image_L1 = convolution2D(image, this_filter)\n",
    "            images_L1.append(image_L1)\n",
    "                    \n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 1: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    # LAYER 2\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    counter_L1 = 0\n",
    "    image_coeffs = []\n",
    "    for image_L1 in images_L1:\n",
    "        \n",
    "        counter_L1 += 1\n",
    "        \n",
    "        bin_image_L1 = np.zeros((shape_1, shape_2))\n",
    "        \n",
    "        for k in range(L2):\n",
    "            this_filter = np.reshape(filters_X_L2[:,k], (k1,k2))\n",
    "            \n",
    "            # output of the convolutional step: N*L1*L2 images\n",
    "            image_L2 = convolution2D(image_L1, this_filter)\n",
    "            \n",
    "            # binarization\n",
    "            bin_image_L2 = binarize(image_L2)\n",
    "            \n",
    "            # concatenation of binaries\n",
    "            bin_image_L1 += bin_image_L2 * 2**k\n",
    "        \n",
    "        # divide each of N*L1 images into B regions\n",
    "        # take histogram\n",
    "        bin_image_L1 = bin_image_L1.astype(int)\n",
    "        \n",
    "        if B == 1:\n",
    "            hist = histogram(bin_image_L1, L2)\n",
    "            image_coeffs.append(hist)\n",
    "        elif B == 9:\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    hist = histogram(bin_image_L1[9*a:9*a+9, 9*b:9*b+9], L2)\n",
    "                    image_coeffs.append(hist)\n",
    "        \n",
    "        # group each set of B*L1*2^L2 coefficients per image\n",
    "        if counter_L1 % L1 == 0:\n",
    "            \n",
    "            # final set of PCANet coeffs\n",
    "            out.append(image_coeffs)\n",
    "            image_coeffs = []\n",
    "            \n",
    "            t1 = time.time()\n",
    "            t += 1\n",
    "            if t % 500 == 0:\n",
    "                print('LAYER 2: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "                t0 = time.time()\n",
    "    \n",
    "    return np.array(out).reshape(N, B*L1*2**L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 1 PCANet filters\n",
    "for i in range(8):\n",
    "    PCANet_filter = np.reshape(PCANet_filters_1[:,i], (7,7))\n",
    "    pylab.imshow(PCANet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2 PCANet filters\n",
    "for i in range(8):\n",
    "    PCANet_filter = np.reshape(PCANet_filters_2[:,i], (7,7))\n",
    "    pylab.imshow(PCANet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "for i in range(8):\n",
    "    PCANet_filter = np.reshape(PCANet_filters_2[:,i] - PCANet_filters_1[:,i], (7,7))\n",
    "    pylab.imshow(PCANet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 0 convolutional filters\n",
    "for i in range(10):\n",
    "    conv_filter = np.loadtxt(\"C:\\\\Users\\\\David\\\\Documents\\\\ETHZ 2015-2017\\\\'16 HERBST\\\\THESIS\\\\R\\\\CNN_layer0_filter\" \\\n",
    "                             + str(i+1) + \".csv\", delimiter=',')\n",
    "    pylab.imshow(conv_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 1 convolutional filters\n",
    "for i in range(10):\n",
    "    conv_filter = np.loadtxt(\"C:\\\\Users\\\\David\\\\Documents\\\\ETHZ 2015-2017\\\\'16 HERBST\\\\THESIS\\\\R\\\\CNN_layer1_filter\" \\\n",
    "                             + str(i+1) + \".csv\", delimiter=',')\n",
    "    pylab.imshow(conv_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 1 RandNet filters\n",
    "for i in range(8):\n",
    "    RandNet_filter = np.reshape(RandNet_filters_1[:,i], (7,7))\n",
    "    pylab.imshow(RandNet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2 RandNet filters\n",
    "for i in range(8):\n",
    "    RandNet_filter = np.reshape(RandNet_filters_2[:,i], (7,7))\n",
    "    pylab.imshow(RandNet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 1 LDANet filters\n",
    "for i in range(8):\n",
    "    LDANet_filter = np.reshape(LDANet_filters_1[:,i], (7,7))\n",
    "    pylab.imshow(LDANet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 2 LDANet filters\n",
    "for i in range(8):\n",
    "    LDANet_filter = np.reshape(LDANet_filters_2[:,i], (7,7)).real\n",
    "    pylab.imshow(LDANet_filter, cmap='gray', interpolation='nearest')\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0: 500 images up to index 500 took: 21.034117937088013 secs!\n",
      "LAYER 0: 500 images up to index 1000 took: 11.044878959655762 secs!\n",
      "LAYER 0: 500 images up to index 1500 took: 11.086468935012817 secs!\n",
      "LAYER 0: 500 images up to index 2000 took: 12.043633222579956 secs!\n",
      "LAYER 0: 500 images up to index 2500 took: 11.021326065063477 secs!\n",
      "LAYER 0: 500 images up to index 3000 took: 12.147176027297974 secs!\n",
      "LAYER 0: 500 images up to index 3500 took: 7.671798944473267 secs!\n",
      "LAYER 0: 500 images up to index 4000 took: 11.484013795852661 secs!\n",
      "LAYER 0: 500 images up to index 4500 took: 17.47793984413147 secs!\n",
      "LAYER 0: 500 images up to index 5000 took: 13.992225885391235 secs!\n",
      "LAYER 0: 500 images up to index 5500 took: 12.173750162124634 secs!\n",
      "LAYER 0: 500 images up to index 6000 took: 16.84982705116272 secs!\n",
      "Cov and eigendecomp 1 took 8.320592880249023 secs!\n",
      "LAYER 1: 500 images up to index 500 took: 142.8670129776001 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 136.09710597991943 secs!\n",
      "LAYER 1: 500 images up to index 1500 took: 151.14039087295532 secs!\n",
      "LAYER 1: 500 images up to index 2000 took: 62.275384187698364 secs!\n",
      "LAYER 1: 500 images up to index 2500 took: 91.37241291999817 secs!\n",
      "LAYER 1: 500 images up to index 3000 took: 117.66568779945374 secs!\n",
      "LAYER 1: 500 images up to index 3500 took: 90.26780605316162 secs!\n",
      "LAYER 1: 500 images up to index 4000 took: 96.43745708465576 secs!\n",
      "LAYER 1: 500 images up to index 4500 took: 118.94226098060608 secs!\n",
      "LAYER 1: 500 images up to index 5000 took: 101.74226093292236 secs!\n",
      "LAYER 1: 500 images up to index 5500 took: 203.6646430492401 secs!\n",
      "LAYER 1: 500 images up to index 6000 took: 148.09169101715088 secs!\n",
      "Cov and eigendecomp 2 took 6770.11621594429 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 41.30674386024475 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 39.88669180870056 secs!\n",
      "LAYER 2: 500 images up to index 1500 took: 38.03769016265869 secs!\n",
      "LAYER 2: 500 images up to index 2000 took: 37.135478019714355 secs!\n",
      "LAYER 2: 500 images up to index 2500 took: 38.33052611351013 secs!\n",
      "LAYER 2: 500 images up to index 3000 took: 38.87496900558472 secs!\n",
      "LAYER 2: 500 images up to index 3500 took: 38.12644290924072 secs!\n",
      "LAYER 2: 500 images up to index 4000 took: 37.05650496482849 secs!\n",
      "LAYER 2: 500 images up to index 4500 took: 36.07507300376892 secs!\n",
      "LAYER 2: 500 images up to index 5000 took: 44.6931688785553 secs!\n",
      "LAYER 2: 500 images up to index 5500 took: 40.507044076919556 secs!\n",
      "LAYER 2: 500 images up to index 6000 took: 34.07733106613159 secs!\n",
      "(6000, 18432)\n"
     ]
    }
   ],
   "source": [
    "RandNet_coeffs_B9, RandNet_filters_1, RandNet_filters_2 = RandNet(images_train[0:6000], B=9)\n",
    "np.savetxt('RandNet_v2.txt', RandNet_coeffs_B9, delimiter=\",\")\n",
    "print(np.shape(RandNet_coeffs_B9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandNet(images, k1=7, k2=7, L1=8, L2=8, B=1):\n",
    "    \n",
    "    out = []\n",
    "    X = []\n",
    "    X_L1 = []\n",
    "    images_L1 = []\n",
    "    pad_1 = np.floor(k1 / 2)\n",
    "    pad_2 = np.floor(k2 / 2)\n",
    "    N = np.shape(images)[0]\n",
    "    shape_1 = np.shape(images)[1]\n",
    "    shape_2 = np.shape(images)[2]\n",
    "    \n",
    "    # LAYER 0\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        padded_image = np.lib.pad(image, ((pad_1,pad_1),(pad_2,pad_2)), 'constant', constant_values=0)\n",
    "        for i in range(shape_1):\n",
    "            for j in range(shape_2):\n",
    "                patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                patch_flat = patch_i_j.flatten()\n",
    "                patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                X.append(patch_mean_rm)\n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 0: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X = np.reshape(X, (N*(shape_1)*(shape_2), k1*k2))\n",
    "    \n",
    "    # random Gaussian filters\n",
    "    filters_X_L1 = np.random.normal(0, 1, (k1*k2, L1))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 1 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 1\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        for k in range(L1):\n",
    "            this_filter = np.reshape(filters_X_L1[:,k], (k1,k2))\n",
    "            image_L1 = convolution2D(image, this_filter)\n",
    "            images_L1.append(image_L1)\n",
    "            padded_image = np.lib.pad(image_L1, ((pad_1,pad_1),(pad_2,pad_2)), 'constant',\n",
    "                                      constant_values=0)\n",
    "            for i in range(shape_1-1):\n",
    "                for j in range(shape_2-1):\n",
    "                    patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                    patch_flat = patch_i_j.flatten()\n",
    "                    patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                    X_L1.append(patch_mean_rm)\n",
    "                    \n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 1: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X_L1 = np.reshape(X_L1, (N*L1*(shape_1-1)*(shape_2-1), k1*k2))\n",
    "    \n",
    "    # random Gaussian filters\n",
    "    filters_X_L2 = np.random.normal(0, 1, (k1*k2, L2))\n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 2 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 2\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    counter_L1 = 0\n",
    "    image_coeffs = []\n",
    "    for image_L1 in images_L1:\n",
    "        \n",
    "        counter_L1 += 1\n",
    "        \n",
    "        bin_image_L1 = np.zeros((shape_1, shape_2))\n",
    "        \n",
    "        for k in range(L2):\n",
    "            this_filter = np.reshape(filters_X_L2[:,k], (k1,k2))\n",
    "            \n",
    "            # output of the convolutional step: N*L1*L2 images\n",
    "            image_L2 = convolution2D(image_L1, this_filter)\n",
    "            \n",
    "            # binarization\n",
    "            bin_image_L2 = binarize(image_L2)\n",
    "            \n",
    "            # concatenation of binaries\n",
    "            bin_image_L1 += bin_image_L2 * 2**k\n",
    "        \n",
    "        # divide each of N*L1 images into B regions\n",
    "        # take histogram\n",
    "        bin_image_L1 = bin_image_L1.astype(int)\n",
    "        \n",
    "        if B == 1:\n",
    "            hist = histogram(bin_image_L1, L2)\n",
    "            image_coeffs.append(hist)\n",
    "        elif B == 9:\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    hist = histogram(bin_image_L1[9*a:9*a+9, 9*b:9*b+9], L2)\n",
    "                    image_coeffs.append(hist)\n",
    "        \n",
    "        # group each set of B*L1*2^L2 coefficients per image\n",
    "        if counter_L1 % L1 == 0:\n",
    "            \n",
    "            # final set of RandNet coeffs\n",
    "            out.append(image_coeffs)\n",
    "            image_coeffs = []\n",
    "            \n",
    "            t1 = time.time()\n",
    "            t += 1\n",
    "            if t % 500 == 0:\n",
    "                print('LAYER 2: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "                t0 = time.time()\n",
    "    \n",
    "    return np.array(out).reshape(N, B*L1*2**L2), filters_X_L1, filters_X_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0: 500 images up to index 500 took: 20.68775701522827 secs!\n",
      "LAYER 0: 500 images up to index 1000 took: 21.951047897338867 secs!\n",
      "LAYER 0: 500 images up to index 1500 took: 12.006227016448975 secs!\n",
      "LAYER 0: 500 images up to index 2000 took: 7.532629013061523 secs!\n",
      "LAYER 0: 500 images up to index 2500 took: 21.851696014404297 secs!\n",
      "LAYER 0: 500 images up to index 3000 took: 23.19837713241577 secs!\n",
      "LAYER 0: 500 images up to index 3500 took: 23.025572776794434 secs!\n",
      "LAYER 0: 500 images up to index 4000 took: 22.68275499343872 secs!\n",
      "LAYER 0: 500 images up to index 4500 took: 20.85108208656311 secs!\n",
      "LAYER 0: 500 images up to index 5000 took: 21.900739192962646 secs!\n",
      "Cov and eigendecomp 1 took 273.6497609615326 secs!\n",
      "LAYER 1: 500 images up to index 500 took: 125.13358283042908 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 115.06117296218872 secs!\n",
      "LAYER 1: 500 images up to index 1500 took: 123.60887908935547 secs!\n",
      "LAYER 1: 500 images up to index 2000 took: 113.6923520565033 secs!\n",
      "LAYER 1: 500 images up to index 2500 took: 68.95129299163818 secs!\n",
      "LAYER 1: 500 images up to index 3000 took: 117.07861304283142 secs!\n",
      "LAYER 1: 500 images up to index 3500 took: 136.876238822937 secs!\n",
      "LAYER 1: 500 images up to index 4000 took: 133.68020009994507 secs!\n",
      "LAYER 1: 500 images up to index 4500 took: 98.2883288860321 secs!\n",
      "LAYER 1: 500 images up to index 5000 took: 92.07937002182007 secs!\n",
      "Cov and eigendecomp 2 took 7405.871171951294 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 92.85729598999023 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 927.1739928722382 secs!\n",
      "LAYER 2: 500 images up to index 1500 took: 61.40417408943176 secs!\n",
      "LAYER 2: 500 images up to index 2000 took: 49.99444890022278 secs!\n",
      "LAYER 2: 500 images up to index 2500 took: 48.326191902160645 secs!\n",
      "LAYER 2: 500 images up to index 3000 took: 51.069632053375244 secs!\n",
      "LAYER 2: 500 images up to index 3500 took: 47.96216893196106 secs!\n",
      "LAYER 2: 500 images up to index 4000 took: 54.42560911178589 secs!\n",
      "LAYER 2: 500 images up to index 4500 took: 47.34528303146362 secs!\n",
      "LAYER 2: 500 images up to index 5000 took: 47.808818101882935 secs!\n",
      "(5000, 18432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:371: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "LDANet_coeffs_train, LDANet_filters_1, LDANet_filters_2 = LDANet_train(images_train[training], \n",
    "                                                                       labels_train[training], B=9)\n",
    "np.savetxt('LDANet_train.txt', LDANet_coeffs_train, delimiter=\",\")\n",
    "print(np.shape(LDANet_coeffs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 1: 500 images up to index 500 took: 5.170669794082642 secs!\n",
      "LAYER 1: 500 images up to index 1000 took: 4.2536680698394775 secs!\n",
      "LAYER 2: 500 images up to index 500 took: 18.6716091632843 secs!\n",
      "LAYER 2: 500 images up to index 1000 took: 23.75978398323059 secs!\n",
      "(1000, 18432)\n"
     ]
    }
   ],
   "source": [
    "# note, testing with all PCANet variants is exactly the same except for different input filters\n",
    "LDANet_coeffs_test = PCANet_test(images_train[validation], LDANet_filters_1, LDANet_filters_2, B=9)\n",
    "np.savetxt('LDANet_test.txt', LDANet_coeffs_test, delimiter=\",\")\n",
    "print(np.shape(LDANet_coeffs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDANet_train(images, labels, k1=7, k2=7, L1=8, L2=8, B=1):\n",
    "    \n",
    "    out = []\n",
    "    X = []\n",
    "    X_L1 = []\n",
    "    images_L1 = []\n",
    "    pad_1 = np.floor(k1 / 2)\n",
    "    pad_2 = np.floor(k2 / 2)\n",
    "    N = np.shape(images)[0]\n",
    "    shape_1 = np.shape(images)[1]\n",
    "    shape_2 = np.shape(images)[2]\n",
    "    \n",
    "    # LAYER 0\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        padded_image = np.lib.pad(image, ((pad_1,pad_1),(pad_2,pad_2)), 'constant', constant_values=0)\n",
    "        for i in range(shape_1):\n",
    "            for j in range(shape_2):\n",
    "                patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                patch_flat = patch_i_j.flatten()\n",
    "                patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                X.append(patch_mean_rm)\n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 0: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X_1 = np.reshape(X, (N*(shape_1)*(shape_2), k1*k2))\n",
    "    labels_1 = np.repeat(labels, (shape_1)*(shape_2))\n",
    "    \n",
    "    # run LDA, extract eigenvectors of solution space\n",
    "    lda_1 = LDA(n_components=L1, store_covariance=True)\n",
    "    lda_1.fit(X_1, labels_1)\n",
    "    \n",
    "    #filters_X_L1 = eigendecomp(lda_1.covariance_, L1)\n",
    "    eigenValues,eigenVectors = np.linalg.eig(lda_1.covariance_)\n",
    "    idx = eigenValues.argsort()[::-1]   \n",
    "    eigenValues = eigenValues[idx]\n",
    "    eigenVectors = eigenVectors[:,idx]\n",
    "    filters_X_L1 = eigenVectors[:,0:L1]\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 1 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 1\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    for image in images:\n",
    "        for k in range(L1):\n",
    "            this_filter = np.reshape(filters_X_L1[:,k], (k1,k2))\n",
    "            image_L1 = convolution2D(image, this_filter)\n",
    "            images_L1.append(image_L1)\n",
    "            padded_image = np.lib.pad(image_L1, ((pad_1,pad_1),(pad_2,pad_2)), 'constant',\n",
    "                                      constant_values=0)\n",
    "            for i in range(shape_1-1):\n",
    "                for j in range(shape_2-1):\n",
    "                    patch_i_j = padded_image[i:i+k1, j:j+k2]\n",
    "                    patch_flat = patch_i_j.flatten()\n",
    "                    patch_mean_rm = patch_flat - patch_flat.mean()\n",
    "                    X_L1.append(patch_mean_rm)\n",
    "                    \n",
    "        t1 = time.time()\n",
    "        t += 1\n",
    "        if t % 500 == 0:\n",
    "            print('LAYER 1: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "            t0 = time.time()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    X_2 = np.reshape(X_L1, (N*L1*(shape_1-1)*(shape_2-1), k1*k2))\n",
    "    labels_2 = np.repeat(labels, L1*(shape_1-1)*(shape_2-1))\n",
    "    \n",
    "    # run LDA, extract eigenvectors of between-class matrix\n",
    "    #lda_2 = LDA(n_components=L2, store_covariance=True)\n",
    "    #lda_2.fit(X_2, labels_2)\n",
    "    #eigenValues2, eigenVectors2 = np.linalg.eig(lda_L2.covariance_)\n",
    "    \n",
    "    # calculate S_W and S_B\n",
    "    S_W = np.zeros((k1*k2, k1*k2))\n",
    "    S_B = np.zeros((k1*k2, k1*k2))\n",
    "    overall_mean = X_2.mean(axis=0)\n",
    "    for i in range(10):\n",
    "        class_indexes = np.where(labels_2 == [i])[0]\n",
    "        class_patches = X_2[class_indexes,]\n",
    "        avg_class_patch = class_patches.mean(axis=0)\n",
    "        class_patches_rm = (class_patches - avg_class_patch)\n",
    "        cov_within = class_patches_rm.T.dot(class_patches_rm)\n",
    "        class_patch_deviation = (avg_class_patch - overall_mean)[np.newaxis]\n",
    "        cov_between = len(class_indexes) * class_patch_deviation.T.dot(class_patch_deviation)\n",
    "        S_W = S_W + cov_within\n",
    "        S_B = S_B + cov_between\n",
    "    \n",
    "    # take (S_W)^(-1) dot S_B\n",
    "    lda_2_cov = np.linalg.inv(S_W).dot(S_B)\n",
    "    \n",
    "    #filters_X_L2 = eigendecomp(lda_2_cov, L2)\n",
    "    eigenValues2, eigenVectors2 = np.linalg.eig(lda_2_cov)\n",
    "    idx = eigenValues2.argsort()[::-1]   \n",
    "    eigenValues2 = eigenValues2[idx]\n",
    "    eigenVectors2 = eigenVectors2[:,idx]\n",
    "    filters_X_L2 = eigenVectors2[:,0:L2]\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Cov and eigendecomp 2 took \" + str(t1-t0) + ' secs!')\n",
    "    \n",
    "    # LAYER 2\n",
    "    t = 0\n",
    "    t0 = time.time()\n",
    "    counter_L1 = 0\n",
    "    image_coeffs = []\n",
    "    for image_L1 in images_L1:\n",
    "        \n",
    "        counter_L1 += 1\n",
    "        \n",
    "        bin_image_L1 = np.zeros((shape_1, shape_2))\n",
    "        \n",
    "        for k in range(L2):\n",
    "            this_filter = np.reshape(filters_X_L2[:,k], (k1,k2))\n",
    "            \n",
    "            # output of the convolutional step: N*L1*L2 images\n",
    "            image_L2 = convolution2D(image_L1, this_filter)\n",
    "            \n",
    "            # binarization\n",
    "            bin_image_L2 = binarize(image_L2)\n",
    "            \n",
    "            # concatenation of binaries\n",
    "            bin_image_L1 += bin_image_L2 * 2**k\n",
    "        \n",
    "        # divide each of N*L1 images into B regions\n",
    "        # take histogram\n",
    "        bin_image_L1 = bin_image_L1.astype(int)\n",
    "        \n",
    "        if B == 1:\n",
    "            hist = histogram(bin_image_L1, L2)\n",
    "            image_coeffs.append(hist)\n",
    "        elif B == 9:\n",
    "            for a in range(3):\n",
    "                for b in range(3):\n",
    "                    hist = histogram(bin_image_L1[9*a:9*a+9, 9*b:9*b+9], L2)\n",
    "                    image_coeffs.append(hist)\n",
    "        \n",
    "        # group each set of B*L1*2^L2 coefficients per image\n",
    "        if counter_L1 % L1 == 0:\n",
    "            \n",
    "            # final set of LDANet coeffs\n",
    "            out.append(image_coeffs)\n",
    "            image_coeffs = []\n",
    "            \n",
    "            t1 = time.time()\n",
    "            t += 1\n",
    "            if t % 500 == 0:\n",
    "                print('LAYER 2: 500 images up to index ' + str(t) + ' took: ' + str(t1-t0) + ' secs!')\n",
    "                t0 = time.time()\n",
    "    \n",
    "    return np.array(out).reshape(N, B*L1*2**L2), filters_X_L1, filters_X_L2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
