{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "\n",
    "def load_mnist(dataset=\"training\", digits=np.arange(10),\n",
    "               path=r'C:\\Users\\David\\Documents\\ETHZ 2015-2017\\'16 HERBST\\THESIS\\MNIST'):\n",
    "    \"\"\"\n",
    "    Loads MNIST files into 3D numpy arrays\n",
    "\n",
    "    Adapted from: http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels.idx1-ubyte')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images.idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels.idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, labels_train = load_mnist(dataset=\"training\")\n",
    "images_test, labels_test = load_mnist(dataset=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels\n",
    "use_labels_train = pd.Series(labels_train.flatten())\n",
    "use_labels_test = pd.Series(labels_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_2704_into_16s():\n",
    "    out = []\n",
    "    for i in range(168):\n",
    "        out += [16 * (i+1)]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_equalization(scattering_vecs):\n",
    "    \n",
    "    robust_scat_vecs = []\n",
    "    \n",
    "    divide_indices = divide_2704_into_16s()\n",
    "    \n",
    "    # list of 5000x16 arrays\n",
    "    X_list = np.split(np.array(scattering_vecs), divide_indices, axis=1)\n",
    "    X = []\n",
    "    \n",
    "    for X_path in X_list:\n",
    "        \n",
    "        # find the signal which produces the biggest energy coeffs in this path\n",
    "        norms = np.linalg.norm(X_path, axis=1)\n",
    "        \n",
    "        const = max(norms)\n",
    "        \n",
    "        X_path = X_path / abs(const)\n",
    "        X += [X_path]\n",
    "        \n",
    "    X = np.concatenate(np.array(X), axis=1)\n",
    "    \n",
    "    for j in range(len(X)):\n",
    "        robust_scat_vecs += [X[j,:]]\n",
    "    \n",
    "    return robust_scat_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize so that all coeffs are in [-1,1]\n",
    "def normalize_scattering_vecs(scattering_vecs, norm_type='hard', per_path=True):\n",
    "    \n",
    "    norm_scat_vecs = []\n",
    "    \n",
    "    # per coeff: make every single coeff in [-1,1]\n",
    "    if not per_path:\n",
    "        \n",
    "        X = np.array(scattering_vecs).T\n",
    "        \n",
    "        # hard normalization\n",
    "        if norm_type == 'hard':\n",
    "            for i in range(len(X)):\n",
    "                if i % 16 == 0:\n",
    "                    dim_i = X[i,:]\n",
    "                    const = max(dim_i.max(), dim_i.min(), key=abs)\n",
    "                    X[i,:] = X[i,:] / abs(const)\n",
    "\n",
    "        # soft normalization\n",
    "        elif norm_type == 'soft':\n",
    "            for i in range(len(X)):\n",
    "                dim_i = X[i,:]\n",
    "\n",
    "                # find mean\n",
    "                mean = dim_i.mean()\n",
    "\n",
    "                # find standard deviation\n",
    "                std = dim_i.std()\n",
    "\n",
    "                # subtract mean, divide by twice standard deviation\n",
    "                X[i,:] = (X[i,:] - mean) / (2 * std)\n",
    "\n",
    "        else:\n",
    "            raise(\"Error: norm_type must be either 'hard' or 'soft'!\")\n",
    "        \n",
    "        X = X.T\n",
    "    \n",
    "    # make every 16 predictors (one path) in [-1,1]\n",
    "    else:\n",
    "        \n",
    "        divide_indices = divide_2704_into_16s()\n",
    "        #divide_indices = divide_8281_into_49s()\n",
    "        \n",
    "        # list of 5000x16 arrays\n",
    "        X_list = np.split(np.array(scattering_vecs), divide_indices, axis=1)\n",
    "        X = []\n",
    "        \n",
    "        # hard normalization\n",
    "        if norm_type == 'hard':\n",
    "            for X_path in X_list:\n",
    "                const = max(np.amax(X_path), np.amin(X_path), key=abs)\n",
    "                X_path = X_path / abs(const)\n",
    "                X += [X_path]\n",
    "        else:\n",
    "            raise(\"Error: norm_type must be 'hard'!\")\n",
    "        \n",
    "        X = np.concatenate(np.array(X), axis=1)\n",
    "    \n",
    "    for j in range(len(X)):\n",
    "        norm_scat_vecs += [X[j,:]]\n",
    "    \n",
    "    return norm_scat_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and preprocess SCNet and PCANet coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_6000 = images_train[:6000]\n",
    "labels_6000 = use_labels_train[:6000]\n",
    "labels_6000 = pd.Series(labels_6000, index=np.arange(6000))\n",
    "labels_5000 = labels_6000[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on SCNet output\n",
    "test_5000 = np.loadtxt('scattering_vecs_5000_sigma08.txt', delimiter=',')\n",
    "test_5000_6000 = np.loadtxt('scattering_vecs_5000_6000_sigma08.txt', delimiter=',')\n",
    "test_6000 = np.reshape(np.append(test_5000, test_5000_6000), (6000,2704))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on perturbed MNIST datasets\n",
    "test_6000_rand = np.loadtxt('rand_scattering_vecs_6000.txt', delimiter=',')\n",
    "test_6000_rot = np.loadtxt('rot_scattering_vecs_6000.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on PCANet\n",
    "test_5000_PCANet_temp = np.loadtxt('PCANet_train_new.txt', delimiter=',')\n",
    "test_5000_6000_PCANet_temp = np.loadtxt('PCANet_test_new.txt', delimiter=',')\n",
    "test_6000_PCANet = np.reshape(np.append(test_5000_PCANet_temp, test_5000_6000_PCANet_temp),\n",
    "                                   (6000, 18432))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_6000_RandNet = np.loadtxt('RandNet_v2.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_5000_LDANet_temp = np.loadtxt('LDANet_train.txt', delimiter=',')\n",
    "test_5000_6000_LDANet_temp = np.loadtxt('LDANet_test.txt', delimiter=',')\n",
    "test_6000_LDANet = np.reshape(np.append(test_5000_LDANet_temp, test_5000_6000_LDANet_temp),\n",
    "                                   (6000, 18432))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_6000_subset = np.random.choice(6000, 1000, replace=False)\n",
    "pd.Series(random_6000_subset).to_csv('random_6000_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_6000_subset = pd.Series.from_csv('random_6000_subset.csv').values\n",
    "validation = random_6000_subset\n",
    "training = np.setdiff1d(np.arange(6000), validation)\n",
    "labels_6000 = use_labels_train[:6000]\n",
    "labels_5000 = labels_6000[training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_5000.to_csv('rand_labels_training_6000.csv')\n",
    "labels_6000[validation].to_csv('rand_labels_validation_6000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_6000 = normalize_scattering_vecs(test_6000, norm_type='hard',\n",
    "                                                per_path=True)\n",
    "norm_scat_vecs_5000 = norm_scat_vecs_6000[:5000]\n",
    "robust_scat_vecs_6000 = robust_equalization(test_6000)\n",
    "robust_scat_vecs_5000 = robust_scat_vecs_6000[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_6000_rand = normalize_scattering_vecs(test_6000_rand, norm_type='hard',\n",
    "                                                     per_path=True)\n",
    "norm_scat_vecs_5000_rand = norm_scat_vecs_6000_rand[:5000]\n",
    "robust_scat_vecs_6000_rand = robust_equalization(test_6000_rand)\n",
    "robust_scat_vecs_5000_rand = robust_scat_vecs_6000_rand[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_6000_rot = normalize_scattering_vecs(test_6000_rot, norm_type='hard',\n",
    "                                                    per_path=True)\n",
    "norm_scat_vecs_5000_rot = norm_scat_vecs_6000_rot[:5000]\n",
    "robust_scat_vecs_6000_rot = robust_equalization(test_6000_rot)\n",
    "robust_scat_vecs_5000_rot = robust_scat_vecs_6000_rot[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_5000_temp = np.array(norm_scat_vecs_6000)[training]\n",
    "robust_scat_vecs_5000_temp = np.array(robust_scat_vecs_6000)[training]\n",
    "norm_scat_vecs_5000 = []\n",
    "robust_scat_vecs_5000 = []\n",
    "for i in range(5000):\n",
    "    norm_scat_vecs_5000 += [norm_scat_vecs_5000_temp[i]]\n",
    "    robust_scat_vecs_5000 += [robust_scat_vecs_5000_temp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_5000_rand_temp = np.array(norm_scat_vecs_6000_rand)[training]\n",
    "robust_scat_vecs_5000_rand_temp = np.array(robust_scat_vecs_6000_rand)[training]\n",
    "norm_scat_vecs_5000_rand = []\n",
    "robust_scat_vecs_5000_rand = []\n",
    "for i in range(5000):\n",
    "    norm_scat_vecs_5000_rand += [norm_scat_vecs_5000_rand_temp[i]]\n",
    "    robust_scat_vecs_5000_rand += [robust_scat_vecs_5000_rand_temp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scat_vecs_5000_rot_temp = np.array(norm_scat_vecs_6000_rot)[training]\n",
    "robust_scat_vecs_5000_rot_temp = np.array(robust_scat_vecs_6000_rot)[training]\n",
    "norm_scat_vecs_5000_rot = []\n",
    "robust_scat_vecs_5000_rot = []\n",
    "for i in range(5000):\n",
    "    norm_scat_vecs_5000_rot += [norm_scat_vecs_5000_rot_temp[i]]\n",
    "    robust_scat_vecs_5000_rot += [robust_scat_vecs_5000_rot_temp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R data files\n",
    "test_6000_imgs = images_train[:6000]\n",
    "test_5000_imgs = np.array(test_6000_imgs)[training]\n",
    "test_5000_6000_imgs = np.array(test_6000_imgs)[validation]\n",
    "\n",
    "pd.DataFrame(test_5000_imgs).to_csv('test_5000_temp.csv')\n",
    "pd.DataFrame(test_5000_6000_imgs).to_csv('test_5000_6000_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R data files for RAND\n",
    "rand_6000_imgs = np.loadtxt('rand_imgs_6000.txt', delimiter=',')\n",
    "rand_5000_imgs = np.array(rand_6000_imgs)[training]\n",
    "rand_5000_6000_imgs = np.array(rand_6000_imgs)[validation]\n",
    "\n",
    "pd.DataFrame(rand_5000_imgs).to_csv('rand_5000_temp.csv')\n",
    "pd.DataFrame(rand_5000_6000_imgs).to_csv('rand_5000_6000_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R data files for ROT\n",
    "rot_6000_imgs = np.loadtxt('rot_imgs_6000.txt', delimiter=',')\n",
    "rot_5000_imgs = np.array(rot_6000_imgs)[training]\n",
    "rot_5000_6000_imgs = np.array(rot_6000_imgs)[validation]\n",
    "\n",
    "pd.DataFrame(rot_5000_imgs).to_csv('rot_5000_temp.csv')\n",
    "pd.DataFrame(rot_5000_6000_imgs).to_csv('rot_5000_6000_temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCANet\n",
    "test_5000_PCANet_temp = np.array(test_6000_PCANet)[training]\n",
    "test_5000_PCANet = []\n",
    "for i in range(5000):\n",
    "    test_5000_PCANet += [test_5000_PCANet_temp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandNet\n",
    "test_5000_RandNet_temp = np.array(test_6000_RandNet)[training]\n",
    "test_5000_RandNet = []\n",
    "for i in range(5000):\n",
    "    test_5000_RandNet += [test_5000_RandNet_temp[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDANet\n",
    "test_5000_LDANet_temp = np.array(test_6000_LDANet)[training]\n",
    "test_5000_LDANet = []\n",
    "for i in range(5000):\n",
    "    test_5000_LDANet += [test_5000_LDANet_temp[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably need some sort of normalization here\n",
    "# d is number of principal components to keep, found by CV\n",
    "def build_PCA_classifier(labels, scattering_vecs, d=100):\n",
    "    \n",
    "    scattering_vecs = pd.Series(scattering_vecs, index=labels.index)\n",
    "    \n",
    "    df = pd.concat({'labels':labels, 'scattering_vecs':scattering_vecs}, axis=1)\n",
    "    groups = df.groupby('labels')\n",
    "    counts = groups.count()\n",
    "    \n",
    "    # find avg scattering vector s_k for each class\n",
    "    avg_class_vecs = groups.sum() / counts\n",
    "    \n",
    "    num_scattering_coeffs = len(avg_class_vecs.T[0].values[0])\n",
    "    \n",
    "    all_pcas = {}\n",
    "    \n",
    "    # run PCA on each class, separate first d components\n",
    "    for i in range(10):\n",
    "        scattering_vecs_i = groups.get_group(i)\n",
    "        \n",
    "        # project onto orthog compl of first d components\n",
    "        pca = PCA(n_components=d)\n",
    "        \n",
    "        # transform series of arrays into single 2-D array\n",
    "        X = np.empty([counts.T[i], num_scattering_coeffs])\n",
    "        j = 0\n",
    "        for k, v in scattering_vecs_i.T.items():\n",
    "            v = v.values[0]\n",
    "            X[j,] = v\n",
    "            j += 1\n",
    "        \n",
    "        # run PCA on the 2-D array of class obs\n",
    "        all_pcas[i] = pca.fit(X)\n",
    "        \n",
    "    # output avg vectors s_k, transformed PCA spaces\n",
    "    return avg_class_vecs, all_pcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA_classifier(s, avg_class_vecs, all_pcas):\n",
    "    \n",
    "    assert(type(s) == np.ndarray)\n",
    "    \n",
    "    deviation = {}\n",
    "    for class_k, vec_k in avg_class_vecs.T.items():\n",
    "        \n",
    "        # avg class scattering vector\n",
    "        s_k = vec_k.values[0]\n",
    "        \n",
    "        # deviation from avg class scattering vector\n",
    "        s_minus_s_k = s - s_k\n",
    "        \n",
    "        # project deviation onto affine approximation space\n",
    "        pca = all_pcas[class_k]\n",
    "        proj_s_minus_s_k = pca.inverse_transform(pca.transform(s_minus_s_k))\n",
    "        \n",
    "        # approximation error vector\n",
    "        approx_error = s_minus_s_k - proj_s_minus_s_k\n",
    "        \n",
    "        # norm of approximation error vector\n",
    "        deviation[class_k] = np.linalg.norm(approx_error)\n",
    "        \n",
    "    # predicted class has avg vec that is closest to s, ignoring first d principal components\n",
    "    deviation = pd.Series(deviation)\n",
    "    class_pred = deviation.idxmin()\n",
    "    \n",
    "    return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PCA(filename='PCA_6000_preds_rob_', labels_train=labels_5000,\n",
    "             vecs_train=robust_scat_vecs_5000, vecs_all=robust_scat_vecs_6000, size=6000, d=50):\n",
    "    \n",
    "    avg_class_vecs, all_pcas = build_PCA_classifier(labels_train, vecs_train, d=d)\n",
    "    \n",
    "    PCA_preds = {}\n",
    "    \n",
    "    for i in range(size):\n",
    "        s = vecs_all[i]\n",
    "        PCA_preds[i] = run_PCA_classifier(s, avg_class_vecs, all_pcas)\n",
    "    \n",
    "    PCA_preds = pd.Series(PCA_preds)\n",
    "    PCA_preds.to_csv(filename + str(d) + '.csv')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier_OVR(labels, scattering_vecs):\n",
    "    \n",
    "    scattering_vecs = pd.Series(scattering_vecs, index=labels.index)\n",
    "    num_scattering_coeffs = len(scattering_vecs.iloc[0])\n",
    "    \n",
    "    # transform series of arrays into single 2-D array\n",
    "    X = np.empty([len(scattering_vecs), num_scattering_coeffs])\n",
    "    j = 0\n",
    "    for k, v in scattering_vecs.T.items():\n",
    "        X[j,] = v\n",
    "        j += 1\n",
    "    \n",
    "    y = labels.values\n",
    "    \n",
    "    #clf = svm.LinearSVC(C=1)  # PCANet, RandNet\n",
    "    clf = svm.LinearSVC(C=0.5)  # LDANet\n",
    "    #clf = OneVsRestClassifier(svm.SVC(C=13, gamma=0.02))  # 6000\n",
    "    #clf = OneVsRestClassifier(svm.SVC(C=20, gamma=0.05))  # rand, 6000\n",
    "    #clf = OneVsRestClassifier(svm.SVC(C=50, gamma=0.01))  # rot, 6000\n",
    "    \n",
    "    return clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SVM(filename='SVM_OVR_preds_6000.csv', labels_train=labels_5000, vecs_train=test_5000_LDANet,\n",
    "             vecs_all=test_6000_LDANet, size=6000):\n",
    "    \n",
    "    clf_SVM_OVR = SVM_classifier_OVR(labels_train, vecs_train)\n",
    "    SVM_OVR_preds = {}\n",
    "    \n",
    "    for i in range(size):\n",
    "        s = vecs_all[i]\n",
    "        SVM_OVR_preds[i] = clf_SVM_OVR.predict(s)[0]\n",
    "    \n",
    "    SVM_OVR_preds = pd.Series(SVM_OVR_preds)\n",
    "    SVM_OVR_preds.to_csv(OVR_filename)\n",
    "    \n",
    "    return SVM_OVR_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_SVM_classifier(classifier, C_grid): #, gamma_grid):\n",
    "    \n",
    "    #assert(type(gamma_grid) == list)\n",
    "    assert(type(C_grid) == list)\n",
    "    \n",
    "    #param_grid = {'C':C_grid, 'gamma':gamma_grid}\n",
    "    param_grid = {'C':C_grid}\n",
    "    \n",
    "    grid_searcher = GridSearchCV(classifier, param_grid, cv=10)\n",
    "    \n",
    "    return grid_searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5}\n",
      "0.9742\n"
     ]
    }
   ],
   "source": [
    "#clf = svm.SVC()\n",
    "clf = svm.LinearSVC()\n",
    "clf.decision_function_shape='ovr'\n",
    "grid_searcher_SVM = CV_SVM_classifier(clf, C_grid=[0.1,0.5,1,2,5,10]) #, gamma_grid=[10,100,1000])\n",
    "grid_searcher_SVM.fit(test_5000_LDANet, labels_5000)\n",
    "print(grid_searcher_SVM.best_params_)\n",
    "print(grid_searcher_SVM.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LASSO_classifier(labels, scattering_vecs, C=1):\n",
    "    \n",
    "    clf_lasso = LogisticRegression(C=C, penalty='l1', tol=0.001)\n",
    "    \n",
    "    return clf_lasso.fit(scattering_vecs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LASSO(filename='LASSO_preds_6000_C=', labels_train=labels_5000,\n",
    "               vecs_train=test_5000_LDANet, vecs_all=test_6000_LDANet, size=6000, C=11):\n",
    "    \n",
    "    clf_LASSO = LASSO_classifier(labels_train, vecs_train, C=C)\n",
    "    LASSO_preds = {}\n",
    "    \n",
    "    for i in range(size):\n",
    "        s = vecs_all[i]\n",
    "        LASSO_preds[i] = clf_LASSO.predict(s)[0]\n",
    "    \n",
    "    LASSO_preds = pd.Series(LASSO_preds)\n",
    "    LASSO_preds.to_csv(filename + str(C) + '.csv')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_LASSO_classifier(classifier, C_grid):\n",
    "    \n",
    "    assert(type(C_grid) == list)\n",
    "    \n",
    "    param_grid = {'C':C_grid}\n",
    "    \n",
    "    grid_searcher = GridSearchCV(classifier, param_grid, cv=10)\n",
    "    \n",
    "    return grid_searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 20}\n",
      "0.9752\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l1', tol=0.001)\n",
    "grid_searcher_LASSO = CV_LASSO_classifier(clf, C_grid=[10,20,40,80])\n",
    "grid_searcher_LASSO.fit(test_5000_LDANet, labels_5000)\n",
    "print(grid_searcher_LASSO.best_params_)\n",
    "print(grid_searcher_LASSO.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(labels, scattering_vecs, n_estimators=100, max_features=50, max_depth=70,\n",
    "                  random_state=0):\n",
    "    \n",
    "    clf_RF = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features,\n",
    "                                    random_state=random_state, min_samples_leaf=1)\n",
    "    \n",
    "    return clf_RF.fit(scattering_vecs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_RF(filename='RF_preds_6000_n', labels_train=labels_5000, vecs_train=robust_scat_vecs_5000,\n",
    "            vecs_all=robust_scat_vecs_6000, size=6000, n_estimators=400, max_features=100,\n",
    "            max_depth=70):\n",
    "    \n",
    "    clf_RF = RF_classifier(labels_train, vecs_train, n_estimators=n_estimators,\n",
    "                           max_features=max_features, max_depth=max_depth, random_state=0)\n",
    "    RF_preds = {}\n",
    "    \n",
    "    for i in range(size):\n",
    "        s = vecs_all[i]\n",
    "        RF_preds[i] = clf_RF.predict(s)[0]\n",
    "    \n",
    "    RF_preds = pd.Series(RF_preds)\n",
    "    RF_preds.to_csv(filename + str(n_estimators) + '.csv')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RF(filename='RF_preds_6000_f80_d70_n', n_estimators=500, max_features=80, max_depth=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_RF_classifier(classifier, n_grid, max_features_grid):\n",
    "    \n",
    "    assert(type(n_grid) == list)\n",
    "    \n",
    "    param_grid = {'n_estimators':n_grid, 'max_features':max_features_grid}\n",
    "    \n",
    "    grid_searcher = GridSearchCV(classifier, param_grid, cv=10)\n",
    "    \n",
    "    return grid_searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "grid_searcher_RF = CV_RF_classifier(clf_rf, n_grid=[600], max_features_grid=[50,100,200,300,400])\n",
    "rid_searcher_RF.fit(test_5000_PCANet_v2, labels_5000)\n",
    "print(grid_searcher_RF.best_params_)\n",
    "print(grid_searcher_RF.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.91660, std: 0.01306, params: {'n_estimators': 500, 'max_features': 40, 'max_depth': 70},\n",
       " mean: 0.91740, std: 0.01384, params: {'n_estimators': 500, 'max_features': 50, 'max_depth': 70},\n",
       " mean: 0.91780, std: 0.01527, params: {'n_estimators': 500, 'max_features': 60, 'max_depth': 70},\n",
       " mean: 0.91780, std: 0.01452, params: {'n_estimators': 500, 'max_features': 70, 'max_depth': 70},\n",
       " mean: 0.91800, std: 0.01515, params: {'n_estimators': 500, 'max_features': 80, 'max_depth': 70},\n",
       " mean: 0.91740, std: 0.01481, params: {'n_estimators': 500, 'max_features': 90, 'max_depth': 70},\n",
       " mean: 0.91780, std: 0.01291, params: {'n_estimators': 500, 'max_features': 100, 'max_depth': 70}]"
      ]
     },
     "execution_count": 36,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "grid_searcher_RF.grid_scores_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}